diff --git a/Makefile b/Makefile
index 39a99d7..d47337d 100644
--- a/Makefile
+++ b/Makefile
@@ -132,6 +132,7 @@ UPROGS=\
 	$U/_grind\
 	$U/_wc\
 	$U/_zombie\
+	$U/_cowtest\
 
 fs.img: mkfs/mkfs README $(UPROGS)
 	mkfs/mkfs fs.img README $(UPROGS)
diff --git a/kernel/defs.h b/kernel/defs.h
index a3c962b..29ea3a9 100644
--- a/kernel/defs.h
+++ b/kernel/defs.h
@@ -63,6 +63,8 @@ void            ramdiskrw(struct buf*);
 void*           kalloc(void);
 void            kfree(void *);
 void            kinit(void);
+int             addrefcnt(void *);
+int             refcnt(void *);
 
 // log.c
 void            initlog(int, struct superblock*);
@@ -147,6 +149,8 @@ void            trapinit(void);
 void            trapinithart(void);
 extern struct spinlock tickslock;
 void            usertrapret(void);
+// COW modification step 4
+void*             cowalloc(pagetable_t, uint64);
 
 // uart.c
 void            uartinit(void);
@@ -173,6 +177,8 @@ uint64          walkaddr(pagetable_t, uint64);
 int             copyout(pagetable_t, uint64, char *, uint64);
 int             copyin(pagetable_t, char *, uint64, uint64);
 int             copyinstr(pagetable_t, char *, uint64, uint64);
+// COW modification step 2
+int             is_cow_page(pagetable_t, uint64);
 
 // plic.c
 void            plicinit(void);
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index 0699e7e..b4c3acd 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -23,10 +23,17 @@ struct {
   struct run *freelist;
 } kmem;
 
+// COW modification step 3
+struct {
+  struct spinlock lock; // needed to avoid multiple processes accessing the same page at same time
+  int refcnt[PHYSTOP/PGSIZE]; // reference count of each page
+} ref;
+
 void
 kinit()
 {
   initlock(&kmem.lock, "kmem");
+  initlock(&ref.lock, "ref");
   freerange(end, (void*)PHYSTOP);
 }
 
@@ -35,8 +42,10 @@ freerange(void *pa_start, void *pa_end)
 {
   char *p;
   p = (char*)PGROUNDUP((uint64)pa_start);
-  for(; p + PGSIZE <= (char*)pa_end; p += PGSIZE)
+  for(; p + PGSIZE <= (char*)pa_end; p += PGSIZE){
+    ref.refcnt[(uint64)p/PGSIZE] = 1; // initialize the reference count of each page to 1
     kfree(p);
+  }
 }
 
 // Free the page of physical memory pointed at by pa,
@@ -51,15 +60,19 @@ kfree(void *pa)
   if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
     panic("kfree");
 
-  // Fill with junk to catch dangling refs.
-  memset(pa, 1, PGSIZE);
-
-  r = (struct run*)pa;
+  acquire(&ref.lock);
+  if(--ref.refcnt[(uint64)pa/PGSIZE] == 0){
+    release(&ref.lock);
+    // Fill with junk to catch dangling refs.
+    memset(pa, 1, PGSIZE);
+    r = (struct run*)pa;
 
-  acquire(&kmem.lock);
-  r->next = kmem.freelist;
-  kmem.freelist = r;
-  release(&kmem.lock);
+    acquire(&kmem.lock);
+    r->next = kmem.freelist;
+    kmem.freelist = r;
+    release(&kmem.lock);
+  } else 
+    release(&ref.lock);
 }
 
 // Allocate one 4096-byte page of physical memory.
@@ -72,11 +85,33 @@ kalloc(void)
 
   acquire(&kmem.lock);
   r = kmem.freelist;
-  if(r)
+  if(r){
     kmem.freelist = r->next;
+    acquire(&ref.lock);
+    ref.refcnt[(uint64)r/PGSIZE] = 1;
+    release(&ref.lock);
+  }
   release(&kmem.lock);
 
   if(r)
     memset((char*)r, 5, PGSIZE); // fill with junk
   return (void*)r;
 }
+
+// get reference count of a page
+int
+refcnt(void *pa){
+  return ref.refcnt[(uint64)pa/PGSIZE];
+}
+
+// add reference count to a page
+int 
+addrefcnt(void *pa){
+  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
+    return -1;
+
+  acquire(&ref.lock);
+  ref.refcnt[(uint64)pa/PGSIZE]++;
+  release(&ref.lock);
+  return 0;
+}
\ No newline at end of file
diff --git a/kernel/riscv.h b/kernel/riscv.h
index 20a01db..f1bbbb4 100644
--- a/kernel/riscv.h
+++ b/kernel/riscv.h
@@ -344,6 +344,8 @@ typedef uint64 *pagetable_t; // 512 PTEs
 #define PTE_X (1L << 3)
 #define PTE_U (1L << 4) // user can access
 
+#define PTE_RSW (1L << 8) // software-readable
+
 // shift a physical address to the right place for a PTE.
 #define PA2PTE(pa) ((((uint64)pa) >> 12) << 10)
 
diff --git a/kernel/trap.c b/kernel/trap.c
index 512c850..ab6d637 100644
--- a/kernel/trap.c
+++ b/kernel/trap.c
@@ -33,6 +33,54 @@ trapinithart(void)
 // handle an interrupt, exception, or system call from user space.
 // called from trampoline.S
 //
+
+
+// copy on write allocating a new page
+void* cowalloc(pagetable_t pagetable, uint64 va){
+  if(va % PGSIZE != 0)  // check if the address is page aligned
+    return 0;
+  uint64 pa = walkaddr(pagetable, va); // get the physical address of the page
+  if(pa == 0) // check if the page is mapped
+    return 0;
+  pte_t *pte = walk(pagetable, va, 0); // get the pte
+  
+  if(refcnt((void*)pa) == 1){ // if the page is not shared
+    *pte |= PTE_W; // set writeable
+    *pte &= ~PTE_RSW; // clear COW
+    return (void*)pa;
+  } else {
+    char *newpage = kalloc(); // allocate a new page
+    if(newpage == 0)
+      return 0;
+
+    memmove(newpage, (char*)pa, PGSIZE); // copy the content of the page
+    *pte &= ~PTE_V; // clear valid
+
+  // uint64 pa_sta = PTE2PA(*pte); // get the start address of the page
+  // uint64 newpage = (uint64)kalloc();
+  // if(newpage == 0)
+  //   return -1;
+  
+    uint64 flags = PTE_FLAGS(*pte); // get flags
+    flags |= PTE_W; // set writeable
+    flags &= ~PTE_RSW; // clear COW
+
+  // uint64 va_sta = PGROUNDDOWN(va); // get the start address of the page
+
+  // memmove((void*)newpage, (void*)pa_sta, PGSIZE); // copy the content of the page
+  // uvmunmap(pagetable, va_sta, PGSIZE, 1); // unmap the page
+  
+    if(mappages(pagetable, va, PGSIZE, (uint64)newpage, flags) !=0){
+      kfree(newpage);
+      *pte |= PTE_V; // set valid
+      return 0;
+    }
+
+    kfree((void*)PGROUNDDOWN(pa)); // free the old page
+    return (void*)newpage;
+  }
+}
+
 void
 usertrap(void)
 {
@@ -65,6 +113,14 @@ usertrap(void)
     intr_on();
 
     syscall();
+  } else if(r_scause() == 15){
+    // page fault
+    uint64 fault_va = r_stval();
+    // if page fault is out of range or is a COW page or is a COW page allocation failed
+    if(fault_va >= p->sz ||
+        is_cow_page(p->pagetable, fault_va) != 0 ||
+        cowalloc(p->pagetable, PGROUNDDOWN(fault_va)) == 0)
+      p->killed = 1;  // out of memory
   } else if((which_dev = devintr()) != 0){
     // ok
   } else {
@@ -83,6 +139,8 @@ usertrap(void)
   usertrapret();
 }
 
+
+
 //
 // return to user space
 //
diff --git a/kernel/vm.c b/kernel/vm.c
index 9f69783..aaf72ae 100644
--- a/kernel/vm.c
+++ b/kernel/vm.c
@@ -302,13 +302,17 @@ uvmfree(pagetable_t pagetable, uint64 sz)
 // physical memory.
 // returns 0 on success, -1 on failure.
 // frees any allocated pages on failure.
+
+// COW modification step 1
 int
 uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
 {
-  pte_t *pte;
-  uint64 pa, i;
-  uint flags;
-  char *mem;
+  pte_t *pte;  // page table entry
+  uint64 pa, i; // physical address
+  uint flags; // PTE flags
+  // char *mem; // physical memory
+
+  // implement copy on write (COW)
 
   for(i = 0; i < sz; i += PGSIZE){
     if((pte = walk(old, i, 0)) == 0)
@@ -317,21 +321,48 @@ uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
       panic("uvmcopy: page not present");
     pa = PTE2PA(*pte);
     flags = PTE_FLAGS(*pte);
-    if((mem = kalloc()) == 0)
-      goto err;
-    memmove(mem, (char*)pa, PGSIZE);
-    if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0){
-      kfree(mem);
+
+    if(flags & PTE_W) {  // read-only page
+      flags = flags | PTE_RSW;
+      flags = flags & ~PTE_W;
+      *pte = PA2PTE(pa) | flags; // update flags to the page table entry
+    }
+
+    // if((mem = kalloc()) == 0)
+    //   goto err;
+    // memmove(mem, (char*)pa, PGSIZE);
+
+    if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){
+      // kfree(mem);
       goto err;
     }
+
+    if(addrefcnt((void*)pa) != 0)
+      return -1;
   }
   return 0;
 
  err:
-  uvmunmap(new, 0, i / PGSIZE, 1);
+  uvmunmap(new, 0, i / PGSIZE, 1);  // unmap pages that have been mapped on failure
   return -1;
 }
 
+// COW modification step 2
+// determine whether it is an unallocated COW page
+// return 0 and -1 otherwise
+int is_cow_page(pagetable_t pagetable, uint64 va) {
+  if(va >= MAXVA)
+    return -1;
+  pte_t *pte = walk(pagetable, va, 0);
+  if (pte == 0) {
+    return -1;
+  }
+  if ((*pte & PTE_V) == 0) {
+    return 0;
+  }
+  return (*pte & PTE_RSW) ? 0 : -1;
+}
+
 // mark a PTE invalid for user access.
 // used by exec for the user stack guard page.
 void
@@ -348,6 +379,7 @@ uvmclear(pagetable_t pagetable, uint64 va)
 // Copy from kernel to user.
 // Copy len bytes from src to virtual address dstva in a given page table.
 // Return 0 on success, -1 on error.
+
 int
 copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
 {
@@ -356,6 +388,12 @@ copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
   while(len > 0){
     va0 = PGROUNDDOWN(dstva);
     pa0 = walkaddr(pagetable, va0);
+
+    if(is_cow_page(pagetable, va0)==0){
+      // if the page is  a COW page, we need to allocate a new page
+      // and copy the content from the old page
+      pa0 = (uint64)cowalloc(pagetable, va0);
+    }
     if(pa0 == 0)
       return -1;
     n = PGSIZE - (dstva - va0);
